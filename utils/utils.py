# -*- coding: utf-8 -*-
"""
å·¥å…·å‡½æ•°æ¨¡å— - åŒ…å«é€šç”¨çš„å·¥å…·å‡½æ•°å’Œè¾…åŠ©æ–¹æ³•
"""

import pandas as pd
import numpy as np
import streamlit as st
from typing import Dict, List, Any, Tuple, Optional, Union

class DataUtils:
    """æ•°æ®å¤„ç†å·¥å…·ç±»"""
    
    @staticmethod
    def load_excel_data(uploaded_file, sheet_name: str) -> pd.DataFrame:
        """
        é«˜æ€§èƒ½åŠ è½½Excelæ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆ - æ”¯æŒç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ–ï¼‰
        
        Args:
            uploaded_file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            sheet_name: Sheetåç§°
            
        Returns:
            pd.DataFrame: æ•°æ®æ¡†
        """
        try:
            # åˆ›å»ºæ–‡ä»¶ç¼“å­˜é”®
            file_key = f"{uploaded_file.name}_{uploaded_file.size}_{sheet_name}"
            cache_key = f"data_{file_key}"
            
            # æ£€æŸ¥ç¼“å­˜
            if cache_key in st.session_state:
                return st.session_state[cache_key]
            
            # æ€§èƒ½ä¼˜åŒ–çš„Excelè¯»å–
            with st.spinner(f"ğŸ“Š æ­£åœ¨é«˜é€ŸåŠ è½½æ•°æ®è¡¨ï¼š{sheet_name}..."):
                # ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿæ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œæ•°æ®é‡
                try:
                    # ä½¿ç”¨openpyxlå¼•æ“ï¼Œæ€§èƒ½æ›´å¥½
                    sample_df = pd.read_excel(
                        uploaded_file, 
                        sheet_name=sheet_name, 
                        nrows=10,
                        engine='openpyxl'  # æ˜ç¡®æŒ‡å®šå¼•æ“
                    )
                    
                    if sample_df.empty:
                        st.warning(f"âš ï¸ å·¥ä½œè¡¨ {sheet_name} ä¸ºç©º")
                        return pd.DataFrame()
                    
                except Exception as e:
                    st.error(f"âŒ æ–‡ä»¶æ ¼å¼æ£€æŸ¥å¤±è´¥: {str(e)}")
                    return pd.DataFrame()
                
                # ç¬¬äºŒæ­¥ï¼šä¼˜åŒ–å‚æ•°è¯»å–å®Œæ•´æ•°æ®
                try:
                    # ä½¿ç”¨ä¼˜åŒ–å‚æ•°åŠ è½½
                    df = pd.read_excel(
                        uploaded_file,
                        sheet_name=sheet_name,
                        engine='openpyxl',  # ä½¿ç”¨openpyxlå¼•æ“ï¼Œé€šå¸¸æ¯”xlrdæ›´å¿«
                        na_values=['', 'NULL', 'null', 'N/A', 'n/a', '#N/A'],  # æ˜ç¡®æŒ‡å®šNAå€¼
                        keep_default_na=True  # ä¿æŒé»˜è®¤NAå¤„ç†
                    )
                    
                    # ç¬¬ä¸‰æ­¥ï¼šæ•°æ®ç±»å‹ä¼˜åŒ–ï¼ˆå‡å°‘å†…å­˜ä½¿ç”¨ï¼‰
                    df = DataUtils._optimize_dataframe_dtypes(df)
                    
                    # ç¼“å­˜æ•°æ®
                    st.session_state[cache_key] = df
                    
                    # æ˜¾ç¤ºåŠ è½½ç»“æœ
                    rows, cols = df.shape
                    file_size = f"{df.memory_usage(deep=True).sum() / 1024:.1f} KB"
                    
                    return df
                    
                except Exception as e:
                    # å¦‚æœä¼˜åŒ–å‚æ•°å¤±è´¥ï¼Œå›é€€åˆ°åŸºæœ¬å‚æ•°
                    st.warning(f"âš ï¸ ä½¿ç”¨åŸºæœ¬æ¨¡å¼åŠ è½½...")
                    df = pd.read_excel(uploaded_file, sheet_name=sheet_name)
                    st.session_state[cache_key] = df
                    return df
                
        except Exception as e:
            st.error(f"âŒ è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    @staticmethod
    def _optimize_dataframe_dtypes(df: pd.DataFrame) -> pd.DataFrame:
        """
        ä¼˜åŒ–DataFrameçš„æ•°æ®ç±»å‹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
        
        Args:
            df: åŸå§‹DataFrame
            
        Returns:
            pd.DataFrame: ä¼˜åŒ–åçš„DataFrame
        """
        try:
            # ä¼˜åŒ–æ•°å€¼åˆ—
            for col in df.select_dtypes(include=['int64']).columns:
                col_min = df[col].min()
                col_max = df[col].max()
                
                # é€‰æ‹©æœ€å°çš„æ•´æ•°ç±»å‹
                if col_min >= -128 and col_max <= 127:
                    df[col] = df[col].astype('int8')
                elif col_min >= -32768 and col_max <= 32767:
                    df[col] = df[col].astype('int16')
                elif col_min >= -2147483648 and col_max <= 2147483647:
                    df[col] = df[col].astype('int32')
            
            # ä¼˜åŒ–æµ®ç‚¹åˆ—
            for col in df.select_dtypes(include=['float64']).columns:
                # æ£€æŸ¥æ˜¯å¦å¯ä»¥è½¬æ¢ä¸ºfloat32è€Œä¸ä¸¢å¤±ç²¾åº¦
                original_values = df[col].dropna()
                if len(original_values) > 0:
                    converted = original_values.astype('float32')
                    if original_values.equals(converted.astype('float64')):
                        df[col] = df[col].astype('float32')
            
            # ä¼˜åŒ–å­—ç¬¦ä¸²åˆ—
            for col in df.select_dtypes(include=['object']).columns:
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ä¸”é‡å¤å€¼å¾ˆå¤šï¼Œè½¬æ¢ä¸ºcategory
                if df[col].dtype == 'object':
                    num_unique_values = df[col].nunique()
                    num_total_values = len(df[col])
                    if num_total_values > 0 and num_unique_values / num_total_values < 0.5:
                        df[col] = df[col].astype('category')
            
            return df
            
        except Exception:
            # å¦‚æœä¼˜åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹DataFrame
            return df
    
    @staticmethod
    def get_excel_sheets_info(uploaded_file) -> Dict[str, Any]:
        """
        è·å–Excelæ–‡ä»¶çš„å·¥ä½œè¡¨ä¿¡æ¯ï¼ˆç¼“å­˜ç‰ˆï¼‰
        
        Args:
            uploaded_file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            
        Returns:
            dict: å·¥ä½œè¡¨ä¿¡æ¯
        """
        try:
            file_key = f"{uploaded_file.name}_{uploaded_file.size}"
            info_key = f"excel_info_{file_key}"
            
            # æ£€æŸ¥ç¼“å­˜
            if info_key in st.session_state:
                return st.session_state[info_key]
            
                                    # è¯»å–Excelæ–‡ä»¶ä¿¡æ¯
            with st.spinner("ğŸ” æ­£åœ¨åˆ†æExcelæ–‡ä»¶ç»“æ„..."):
                xls = pd.ExcelFile(uploaded_file)
                sheet_names = xls.sheet_names
                
                # è·å–æ¯ä¸ªsheetçš„åŸºæœ¬ä¿¡æ¯
                sheets_info = {}
                for sheet_name in sheet_names:
                    try:
                        # åªè¯»å–å‰å‡ è¡Œæ¥è·å–åŸºæœ¬ä¿¡æ¯
                        sample_df = pd.read_excel(uploaded_file, sheet_name=sheet_name, nrows=10)
                        sheets_info[sheet_name] = {
                            'columns': len(sample_df.columns),
                            'has_data': not sample_df.empty,
                            'sample_columns': list(sample_df.columns)[:5] if not sample_df.empty else []  # å‰5åˆ—
                        }
                    except Exception:
                        sheets_info[sheet_name] = {
                            'columns': 0,
                            'has_data': False,
                            'sample_columns': []
                        }
                    
                info = {
                    'sheet_names': sheet_names,
                    'sheet_count': len(sheet_names),
                    'sheets_info': sheets_info
                }
                
                # ç¼“å­˜ä¿¡æ¯
                st.session_state[info_key] = info
                
                return info
                
        except Exception as e:
            st.error(f"âŒ Excelæ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}")
            return {'sheet_names': [], 'sheet_count': 0, 'sheets_info': {}}
    
    @staticmethod
    def get_excel_sheets_names_only(uploaded_file) -> Dict[str, Any]:
        """
        ä»…è·å–Excelæ–‡ä»¶çš„å·¥ä½œè¡¨åç§°ï¼ˆå¿«é€Ÿç‰ˆæœ¬ï¼‰
        
        Args:
            uploaded_file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            
        Returns:
            dict: ä»…åŒ…å«å·¥ä½œè¡¨åç§°çš„ä¿¡æ¯
        """
        try:
            file_key = f"{uploaded_file.name}_{uploaded_file.size}"
            names_key = f"excel_names_{file_key}"
            
            # æ£€æŸ¥ç¼“å­˜
            if names_key in st.session_state:
                return st.session_state[names_key]
            
            # ä»…è¯»å–å·¥ä½œè¡¨åç§°ï¼Œä¸è¯»å–ä»»ä½•å†…å®¹
            with st.spinner("ğŸ” æ­£åœ¨è¯»å–å·¥ä½œè¡¨åç§°..."):
                xls = pd.ExcelFile(uploaded_file)
                sheet_names = xls.sheet_names
                
                info = {
                    'sheet_names': sheet_names,
                    'sheet_count': len(sheet_names)
                }
                
                # ç¼“å­˜ä¿¡æ¯
                st.session_state[names_key] = info
                
                return info
                
        except Exception as e:
            st.error(f"âŒ è¯»å–Excelå·¥ä½œè¡¨åç§°å¤±è´¥: {str(e)}")
            return {'sheet_names': [], 'sheet_count': 0}
    
    @staticmethod
    def load_data_in_background(uploaded_file, sheet_name: str, progress_placeholder=None):
        """
        åœ¨åå°åŠ è½½æ•°æ®ï¼ˆå¸¦è¿›åº¦æç¤ºï¼‰
        
        Args:
            uploaded_file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            sheet_name: Sheetåç§°
            progress_placeholder: è¿›åº¦æ˜¾ç¤ºå ä½ç¬¦
            
        Returns:
            pd.DataFrame: æ•°æ®æ¡†
        """
        try:
            # åˆ›å»ºæ–‡ä»¶ç¼“å­˜é”®
            file_key = f"{uploaded_file.name}_{uploaded_file.size}_{sheet_name}"
            cache_key = f"data_{file_key}"
            
            # æ£€æŸ¥ç¼“å­˜
            if cache_key in st.session_state:
                if progress_placeholder:
                    progress_placeholder.success("ğŸ“‹ ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼ŒåŠ è½½å®Œæˆï¼")
                return st.session_state[cache_key]
            
            # æ˜¾ç¤ºè¯¦ç»†çš„åŠ è½½è¿›åº¦
            if progress_placeholder:
                progress_placeholder.info(f"ğŸ”„ æ­£åœ¨è¯»å–å·¥ä½œè¡¨ï¼š{sheet_name}")
            
            # å…ˆå°è¯•è¯»å–å°‘é‡æ•°æ®æ£€æŸ¥æ–‡ä»¶æ ¼å¼
            try:
                sample_df = pd.read_excel(uploaded_file, sheet_name=sheet_name, nrows=5)
                if sample_df.empty:
                    if progress_placeholder:
                        progress_placeholder.warning(f"âš ï¸ å·¥ä½œè¡¨ {sheet_name} ä¸ºç©º")
                    return pd.DataFrame()
                
                total_rows = len(pd.read_excel(uploaded_file, sheet_name=sheet_name))
                
                if progress_placeholder:
                    progress_placeholder.info(f"ğŸ“Š æ£€æµ‹åˆ° {total_rows:,} è¡Œæ•°æ®ï¼Œæ­£åœ¨åŠ è½½...")
                
            except Exception as e:
                if progress_placeholder:
                    progress_placeholder.error(f"âŒ æ–‡ä»¶æ ¼å¼æ£€æŸ¥å¤±è´¥: {str(e)}")
                return pd.DataFrame()
            
            # è¯»å–å®Œæ•´æ•°æ®
            df = pd.read_excel(uploaded_file, sheet_name=sheet_name)
            
            # ç¼“å­˜æ•°æ®
            st.session_state[cache_key] = df
            
            # æ˜¾ç¤ºåŠ è½½ç»“æœ
            rows, cols = df.shape
            file_size = f"{df.memory_usage(deep=True).sum() / 1024:.1f} KB"
            
            if progress_placeholder:
                progress_placeholder.success(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼{rows:,} è¡Œ Ã— {cols} åˆ—ï¼Œå ç”¨å†…å­˜: {file_size}")
            
            return df
                
        except Exception as e:
            error_msg = f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}"
            if progress_placeholder:
                progress_placeholder.error(error_msg)
            else:
                st.error(error_msg)
            return pd.DataFrame()
    
    @staticmethod
    def validate_columns_existence(df: pd.DataFrame, required_columns: List[str]) -> Tuple[bool, List[str]]:
        """
        éªŒè¯æ•°æ®æ¡†ä¸­æ˜¯å¦å­˜åœ¨å¿…éœ€çš„åˆ—
        
        Args:
            df: æ•°æ®æ¡†
            required_columns: å¿…éœ€çš„åˆ—ååˆ—è¡¨
            
        Returns:
            tuple: (æ˜¯å¦å…¨éƒ¨å­˜åœ¨, ç¼ºå¤±çš„åˆ—ååˆ—è¡¨)
        """
        missing_columns = [col for col in required_columns if col not in df.columns]
        return len(missing_columns) == 0, missing_columns
    
    @staticmethod 
    def clean_numeric_column(series: pd.Series, column_name: str = "æ•°æ®") -> pd.Series:
        """
        æ¸…ç†æ•°å€¼åˆ—ï¼Œè½¬æ¢ä¸ºæ•°å€¼ç±»å‹å¹¶å¤„ç†å¼‚å¸¸å€¼
        
        Args:
            series: æ•°æ®åºåˆ—
            column_name: åˆ—åï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            pd.Series: æ¸…ç†åçš„æ•°æ®åºåˆ—
        """
        original_count = len(series)
        
        # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
        numeric_series = pd.to_numeric(series, errors='coerce')
        
        # ç»Ÿè®¡è½¬æ¢å¤±è´¥çš„æ•°é‡
        failed_count = numeric_series.isna().sum() - series.isna().sum()
        if failed_count > 0:
            st.warning(f"{column_name}åˆ—æœ‰ {failed_count} ä¸ªå€¼æ— æ³•è½¬æ¢ä¸ºæ•°å€¼")
            
        return numeric_series
    
    @staticmethod
    def get_column_info(df: pd.DataFrame) -> Dict[str, Any]:
        """
        è·å–æ•°æ®æ¡†çš„åˆ—ä¿¡æ¯
        
        Args:
            df: æ•°æ®æ¡†
            
        Returns:
            dict: åˆ—ä¿¡æ¯ç»Ÿè®¡
        """
        info = {
            'total_columns': len(df.columns),
            'numeric_columns': len(df.select_dtypes(include=[np.number]).columns),
            'text_columns': len(df.select_dtypes(include=['object', 'string']).columns),
            'datetime_columns': len(df.select_dtypes(include=['datetime']).columns),
            'columns_with_nulls': df.isnull().any().sum(),
            'total_null_values': df.isnull().sum().sum()
        }
        return info
    
    @staticmethod
    def filter_valid_rows(df: pd.DataFrame, required_columns: List[str]) -> pd.DataFrame:
        """
        è¿‡æ»¤æ‰åŒ…å«å¿…éœ€åˆ—ä¸­æœ‰ç©ºå€¼çš„è¡Œ
        
        Args:
            df: æ•°æ®æ¡†
            required_columns: å¿…éœ€çš„åˆ—ååˆ—è¡¨
            
        Returns:
            pd.DataFrame: è¿‡æ»¤åçš„æ•°æ®æ¡†
        """
        before_count = len(df)
        
        # è¿‡æ»¤ç©ºå€¼
        mask = df[required_columns].notna().all(axis=1)
        filtered_df = df[mask].copy()
        
        after_count = len(filtered_df)
        if before_count != after_count:
            st.info(f"è¿‡æ»¤æ‰ {before_count - after_count} è¡ŒåŒ…å«ç©ºå€¼çš„æ•°æ®ï¼Œå‰©ä½™ {after_count} è¡Œ")
            
        return filtered_df

class SessionStateManager:
    """SessionçŠ¶æ€ç®¡ç†å™¨"""
    
    @staticmethod
    def initialize_session_state():
        """åˆå§‹åŒ–SessionçŠ¶æ€"""
        default_states = {
            'analysis_type': None,
            'analysis_name': None,
            'sheet_confirmed': False,
            'selected_sheet': None,
            'dimensions_confirmed': False,
            'selected_dimensions': [],
            'analysis_confirmed': False,
            'data_loaded': False,
            'container_length': 600,
            'container_width': 400,
            'container_height': 300
        }
        
        for key, default_value in default_states.items():
            if key not in st.session_state:
                st.session_state[key] = default_value
    
    @staticmethod
    def clear_session_data(keys_to_clear: Optional[List[str]] = None):
        """
        æ¸…ç†Sessionæ•°æ®
        
        Args:
            keys_to_clear: è¦æ¸…ç†çš„é”®åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…ç†æ‰€æœ‰æ•°æ®ç¼“å­˜
        """
        if keys_to_clear is None:
            keys_to_clear = [
                'sheet_confirmed', 'analysis_type', 'dimensions_confirmed', 
                'analysis_confirmed', 'selected_dimensions', 'analysis_name', 
                'data_loaded', 'loaded_data', 'need_data_loading', 'data_loading_progress',
                'selected_sheet', 'uploaded_file', 'dimension_configs',
                'data_loading_error', 'data_loading_triggered', 'loading_triggered'
            ]
        
        for key in keys_to_clear:
            if key in st.session_state:
                del st.session_state[key]
        
                # æ¸…ç†æ•°æ®ç¼“å­˜ï¼ˆæ‰€æœ‰ä»¥data_å¼€å¤´çš„é”®ï¼‰
        for key in list(st.session_state.keys()):
            if isinstance(key, str) and key.startswith('data_'):
                del st.session_state[key]
        
        # æ¸…ç†åŠ è½½çŠ¶æ€ç›¸å…³çš„é”®
        loading_keys = [key for key in st.session_state.keys() if 'loading' in str(key).lower() or 'current_sheet' in str(key).lower()]
        for key in loading_keys:
            if key in st.session_state:
                del st.session_state[key]
        
        # æ¸…ç†åˆ†æé…ç½®ç›¸å…³çš„é”®
        config_keys = [
            key for key in st.session_state.keys() 
            if any(prefix in str(key) for prefix in ['è£…ç®±åˆ†æ_', 'ABCåˆ†æ_', 'å¼‚å¸¸æ•°æ®æ¸…æ´—_', 'å‡ºåº“åˆ†æ_', 'å…¥åº“åˆ†æ_'])
        ]
        for key in config_keys:
            if key in st.session_state:
                del st.session_state[key]
    
    @staticmethod
    def get_analysis_config(dimension: str) -> Dict[str, Any]:
        """
        è·å–ç‰¹å®šç»´åº¦çš„åˆ†æé…ç½®
        
        Args:
            dimension: åˆ†æç»´åº¦åç§°
            
        Returns:
            dict: é…ç½®ä¿¡æ¯
        """
        config = {}
        
        # è£…ç®±åˆ†æé…ç½®
        if dimension == "è£…ç®±åˆ†æ":
            config = {
                'length_column': st.session_state.get("è£…ç®±åˆ†æ_length_column"),
                'width_column': st.session_state.get("è£…ç®±åˆ†æ_width_column"),
                'height_column': st.session_state.get("è£…ç®±åˆ†æ_height_column"),
                'inventory_column': st.session_state.get("è£…ç®±åˆ†æ_inventory_column"),
                'weight_column': st.session_state.get("è£…ç®±åˆ†æ_weight_column"),
                'data_unit': st.session_state.get("è£…ç®±åˆ†æ_data_unit", "cm"),
                'weight_unit': st.session_state.get("è£…ç®±åˆ†æ_weight_unit", "kg"),
                'show_details': st.session_state.get("è£…ç®±åˆ†æ_show_details", True),
                'container_length': st.session_state.get("container_length", 600),
                'container_width': st.session_state.get("container_width", 400),
                'container_height': st.session_state.get("container_height", 300),
                'container_weight_limit': st.session_state.get("container_weight_limit", 30),
                'use_dividers': st.session_state.get("use_dividers") == "æ˜¯",
                'selected_dividers': st.session_state.get("selected_dividers", [])
            }
        
        # å¼‚å¸¸æ•°æ®æ¸…æ´—é…ç½®
        elif dimension == "å¼‚å¸¸æ•°æ®æ¸…æ´—":
            config = {
                'all_conditions': st.session_state.get("å¼‚å¸¸æ•°æ®æ¸…æ´—_all_conditions", []),
                'overall_logic': st.session_state.get("å¼‚å¸¸æ•°æ®æ¸…æ´—_overall_logic", "OR")
            }
        
        # ABCåˆ†æé…ç½®
        elif dimension == "ABCåˆ†æ":
            config = {
                'sku_column': st.session_state.get("ABCåˆ†æ_sku_column"),
                'quantity_column': st.session_state.get("ABCåˆ†æ_quantity_column"),
                'a_percentage': st.session_state.get("ABCåˆ†æ_a_percentage", 80),
                'b_percentage': st.session_state.get("ABCåˆ†æ_b_percentage", 15)
            }
        
        # å‡ºåº“åˆ†æé…ç½®
        elif dimension == "å‡ºåº“åˆ†æ":
            config = {
                'å‡ºåº“åˆ†æ_date_column': st.session_state.get("å‡ºåº“åˆ†æ_date_column"),
                'å‡ºåº“åˆ†æ_order_data_type': st.session_state.get("å‡ºåº“åˆ†æ_order_data_type"),
                'å‡ºåº“åˆ†æ_order_id_column': st.session_state.get("å‡ºåº“åˆ†æ_order_id_column"),
                'å‡ºåº“åˆ†æ_order_count_column': st.session_state.get("å‡ºåº“åˆ†æ_order_count_column"),
                'å‡ºåº“åˆ†æ_sku_data_type': st.session_state.get("å‡ºåº“åˆ†æ_sku_data_type"),
                'å‡ºåº“åˆ†æ_sku_column': st.session_state.get("å‡ºåº“åˆ†æ_sku_column"),
                'å‡ºåº“åˆ†æ_sku_count_column': st.session_state.get("å‡ºåº“åˆ†æ_sku_count_column"),
                'å‡ºåº“åˆ†æ_item_data_type': st.session_state.get("å‡ºåº“åˆ†æ_item_data_type"),
                'å‡ºåº“åˆ†æ_item_column': st.session_state.get("å‡ºåº“åˆ†æ_item_column"),
                'å‡ºåº“åˆ†æ_item_count_column': st.session_state.get("å‡ºåº“åˆ†æ_item_count_column"),
                'å‡ºåº“åˆ†æ_start_date': st.session_state.get("å‡ºåº“åˆ†æ_start_date"),
                'å‡ºåº“åˆ†æ_end_date': st.session_state.get("å‡ºåº“åˆ†æ_end_date")
            }
        
        # å…¥åº“åˆ†æé…ç½®
        elif dimension == "å…¥åº“åˆ†æ":
            config = {
                'å…¥åº“åˆ†æ_date_column': st.session_state.get("å…¥åº“åˆ†æ_date_column"),
                'å…¥åº“åˆ†æ_sku_data_type': st.session_state.get("å…¥åº“åˆ†æ_sku_data_type"),
                'å…¥åº“åˆ†æ_sku_column': st.session_state.get("å…¥åº“åˆ†æ_sku_column"),
                'å…¥åº“åˆ†æ_sku_count_column': st.session_state.get("å…¥åº“åˆ†æ_sku_count_column"),
                'å…¥åº“åˆ†æ_quantity_data_type': st.session_state.get("å…¥åº“åˆ†æ_quantity_data_type"),
                'å…¥åº“åˆ†æ_quantity_column': st.session_state.get("å…¥åº“åˆ†æ_quantity_column"),
                'å…¥åº“åˆ†æ_quantity_count_column': st.session_state.get("å…¥åº“åˆ†æ_quantity_count_column"),
                'å…¥åº“åˆ†æ_start_date': st.session_state.get("å…¥åº“åˆ†æ_start_date"),
                'å…¥åº“åˆ†æ_end_date': st.session_state.get("å…¥åº“åˆ†æ_end_date")
            }
        

        
        # è®¢å•ç»“æ„åˆ†æé…ç½®
        elif dimension == "è®¢å•ç»“æ„åˆ†æ":
            config = {
                'è®¢å•ç»“æ„åˆ†æ_order_column': st.session_state.get("è®¢å•ç»“æ„åˆ†æ_order_column"),
                'è®¢å•ç»“æ„åˆ†æ_item_column': st.session_state.get("è®¢å•ç»“æ„åˆ†æ_item_column"),
                'è®¢å•ç»“æ„åˆ†æ_quantity_column': st.session_state.get("è®¢å•ç»“æ„åˆ†æ_quantity_column"),
                'è®¢å•ç»“æ„åˆ†æ_amount_column': st.session_state.get("è®¢å•ç»“æ„åˆ†æ_amount_column")
            }
        
        return config

class FileUtils:
    """æ–‡ä»¶å¤„ç†å·¥å…·ç±»"""
    
    @staticmethod
    def generate_filename(prefix: str, analysis_type: str = "", extension: str = "csv") -> str:
        """
        ç”Ÿæˆæ–‡ä»¶å
        
        Args:
            prefix: æ–‡ä»¶åå‰ç¼€
            analysis_type: åˆ†æç±»å‹
            extension: æ–‡ä»¶æ‰©å±•å
            
        Returns:
            str: ç”Ÿæˆçš„æ–‡ä»¶å
        """
        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
        
        if analysis_type:
            filename = f"{prefix}_{analysis_type}_{timestamp}.{extension}"
        else:
            filename = f"{prefix}_{timestamp}.{extension}"
            
        return filename
    
    @staticmethod
    def convert_df_to_csv(df: pd.DataFrame, include_index: bool = False) -> bytes:
        """
        å°†DataFrameè½¬æ¢ä¸ºCSVå­—èŠ‚æ•°æ®
        
        Args:
            df: æ•°æ®æ¡†
            include_index: æ˜¯å¦åŒ…å«ç´¢å¼•
            
        Returns:
            bytes: CSVå­—èŠ‚æ•°æ®
        """
        return df.to_csv(index=include_index).encode('utf-8-sig')
    
    @staticmethod
    def prepare_download_data(data: Dict[str, Any], format_type: str = "csv") -> bytes:
        """
        å‡†å¤‡ä¸‹è½½æ•°æ®
        
        Args:
            data: è¦å¯¼å‡ºçš„æ•°æ®å­—å…¸
            format_type: æ ¼å¼ç±»å‹
            
        Returns:
            bytes: æ ¼å¼åŒ–åçš„æ•°æ®
        """
        if format_type == "csv":
            # å¦‚æœæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºDataFrame
            if isinstance(data, dict):
                df = pd.DataFrame([data]).T
                df.columns = ['æ•°å€¼']
                return FileUtils.convert_df_to_csv(df, include_index=True)
            elif isinstance(data, pd.DataFrame):
                return FileUtils.convert_df_to_csv(data)
            else:
                # å…¶ä»–ç±»å‹è½¬ä¸ºæ–‡æœ¬
                text_data = str(data)
                return text_data.encode('utf-8-sig')
        
        return b""

class ValidationUtils:
    """æ•°æ®éªŒè¯å·¥å…·ç±»"""
    
    @staticmethod
    def validate_positive_number(value: Any, field_name: str = "æ•°å€¼") -> Tuple[bool, str]:
        """
        éªŒè¯æ˜¯å¦ä¸ºæ­£æ•°
        
        Args:
            value: è¦éªŒè¯çš„å€¼
            field_name: å­—æ®µåç§°
            
        Returns:
            tuple: (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        try:
            num_value = float(value)
            if num_value <= 0:
                return False, f"{field_name}å¿…é¡»å¤§äº0"
            return True, ""
        except (ValueError, TypeError):
            return False, f"{field_name}å¿…é¡»æ˜¯æœ‰æ•ˆæ•°å­—"
    
    @staticmethod
    def validate_dimension_data(length: Any, width: Any, height: Any) -> Tuple[bool, List[str]]:
        """
        éªŒè¯å°ºå¯¸æ•°æ®
        
        Args:
            length, width, height: é•¿å®½é«˜æ•°æ®
            
        Returns:
            tuple: (æ˜¯å¦å…¨éƒ¨æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯åˆ—è¡¨)
        """
        errors = []
        
        # éªŒè¯é•¿åº¦
        is_valid, error = ValidationUtils.validate_positive_number(length, "é•¿åº¦")
        if not is_valid:
            errors.append(error)
            
        # éªŒè¯å®½åº¦
        is_valid, error = ValidationUtils.validate_positive_number(width, "å®½åº¦")
        if not is_valid:
            errors.append(error)
            
        # éªŒè¯é«˜åº¦
        is_valid, error = ValidationUtils.validate_positive_number(height, "é«˜åº¦")
        if not is_valid:
            errors.append(error)
            
        return len(errors) == 0, errors
    
    @staticmethod
    def check_data_reasonableness(df: pd.DataFrame, dimension_columns: List[str], 
                                unit: str = "cm") -> List[str]:
        """
        æ£€æŸ¥æ•°æ®åˆç†æ€§
        
        Args:
            df: æ•°æ®æ¡†
            dimension_columns: å°ºå¯¸åˆ—ååˆ—è¡¨
            unit: æ•°æ®å•ä½
            
        Returns:
            list: è­¦å‘Šä¿¡æ¯åˆ—è¡¨
        """
        warnings = []
        
        # å•ä½è½¬æ¢å› å­
        unit_factors = {"mm": 1, "cm": 10, "m": 1000}
        factor = unit_factors.get(unit, 10)
        
        for col in dimension_columns:
            if col in df.columns:
                data = pd.to_numeric(df[col], errors='coerce')
                data_mm = data * factor
                
                # æ£€æŸ¥å¼‚å¸¸å°çš„å€¼
                too_small = (data_mm < 1).sum()
                if too_small > 0:
                    warnings.append(f"åˆ—'{col}'æœ‰{too_small}ä¸ªå€¼å°äº1mmï¼Œå¯èƒ½å­˜åœ¨æ•°æ®é—®é¢˜")
                
                # æ£€æŸ¥å¼‚å¸¸å¤§çš„å€¼
                too_large = (data_mm > 10000).sum()  # å¤§äº10ç±³
                if too_large > 0:
                    warnings.append(f"åˆ—'{col}'æœ‰{too_large}ä¸ªå€¼å¤§äº10ç±³ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®é—®é¢˜")
                
                # æ£€æŸ¥é›¶å€¼æˆ–è´Ÿå€¼
                invalid = (data_mm <= 0).sum()
                if invalid > 0:
                    warnings.append(f"åˆ—'{col}'æœ‰{invalid}ä¸ªæ— æ•ˆå€¼ï¼ˆâ‰¤0ï¼‰")
        
        return warnings

class ProgressUtils:
    """è¿›åº¦æ˜¾ç¤ºå·¥å…·ç±»"""
    
    @staticmethod
    def create_progress_bar(total_steps: int, description: str = "å¤„ç†ä¸­..."):
        """
        åˆ›å»ºè¿›åº¦æ¡
        
        Args:
            total_steps: æ€»æ­¥æ•°
            description: æè¿°æ–‡å­—
            
        Returns:
            streamlit progress object
        """
        progress_bar = st.progress(0)
        status_text = st.empty()
        status_text.text(f"{description} (0/{total_steps})")
        
        return progress_bar, status_text
    
    @staticmethod
    def update_progress(progress_bar, status_text, current_step: int, 
                       total_steps: int, description: str = "å¤„ç†ä¸­..."):
        """
        æ›´æ–°è¿›åº¦æ¡
        
        Args:
            progress_bar: è¿›åº¦æ¡å¯¹è±¡
            status_text: çŠ¶æ€æ–‡æœ¬å¯¹è±¡
            current_step: å½“å‰æ­¥æ•°
            total_steps: æ€»æ­¥æ•°
            description: æè¿°æ–‡å­—
        """
        progress = current_step / total_steps
        progress_bar.progress(progress)
        status_text.text(f"{description} ({current_step}/{total_steps})")
    
    @staticmethod
    def complete_progress(progress_bar, status_text, description: str = "å®Œæˆï¼"):
        """
        å®Œæˆè¿›åº¦æ˜¾ç¤º
        
        Args:
            progress_bar: è¿›åº¦æ¡å¯¹è±¡
            status_text: çŠ¶æ€æ–‡æœ¬å¯¹è±¡
            description: å®Œæˆæè¿°
        """
        progress_bar.progress(1.0)
        status_text.text(description)

class FormatUtils:
    """æ ¼å¼åŒ–å·¥å…·ç±»"""
    
    @staticmethod
    def format_number(value: float, decimal_places: int = 2, use_separator: bool = True) -> str:
        """
        æ ¼å¼åŒ–æ•°å­—
        
        Args:
            value: æ•°å€¼
            decimal_places: å°æ•°ä½æ•°
            use_separator: æ˜¯å¦ä½¿ç”¨åƒåˆ†ä½åˆ†éš”ç¬¦
            
        Returns:
            str: æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
        """
        if pd.isna(value) or value == float('inf') or value == float('-inf'):
            return "N/A"
            
        if use_separator:
            return f"{value:,.{decimal_places}f}"
        else:
            return f"{value:.{decimal_places}f}"
    
    @staticmethod
    def format_percentage(value: float, decimal_places: int = 1) -> str:
        """
        æ ¼å¼åŒ–ç™¾åˆ†æ¯”
        
        Args:
            value: æ•°å€¼ï¼ˆ0-1ä¹‹é—´ï¼‰
            decimal_places: å°æ•°ä½æ•°
            
        Returns:
            str: æ ¼å¼åŒ–åçš„ç™¾åˆ†æ¯”å­—ç¬¦ä¸²
        """
        if pd.isna(value):
            return "N/A"
        
        return f"{value * 100:.{decimal_places}f}%"
    
    @staticmethod
    def format_file_size(size_bytes: int) -> str:
        """
        æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        
        Args:
            size_bytes: å­—èŠ‚æ•°
            
        Returns:
            str: æ ¼å¼åŒ–åçš„æ–‡ä»¶å¤§å°
        """
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        elif size_bytes < 1024 * 1024 * 1024:
            return f"{size_bytes / (1024 * 1024):.1f} MB"
        else:
            return f"{size_bytes / (1024 * 1024 * 1024):.1f} GB" 